{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import least_squares,minimize\n",
    "from scipy.integrate import simpson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA = 6.02214076E23  # Avogadro's number\n",
    "Ntrue = 188 #Number of ions used\n",
    "k = 1.0457  # units: kJ/mol/nm^2 - force constant being used in calculations\n",
    "R = 8.31446261815324  # units: J/(mol*K) - ideal gas constant\n",
    "T = 300  # units: K - temperature used for calculations\n",
    "L_x, L_y, L_z = 4.8, 4.8, 14.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alirezza SI nm-based answers\n",
    "# print(1.9247 / (conversion_factor ** (-0.5)))\n",
    "# print(9.3393E-2 / (conversion_factor ** (-1)))\n",
    "# print(3.4572E-2 / (conversion_factor **(-2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting initial concentration profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_c_profile(xvg_file):\n",
    "    \"\"\"\n",
    "    Computes a concentration profile from an XVG file by averaging symmetric parts of the profile.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xvg_file : str\n",
    "        The path to the XVG file containing the number density data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    z_coords : numpy.ndarray\n",
    "        The z-coordinates of the concentration profile.\n",
    "    c_profile : numpy.ndarray\n",
    "        The symmetric concentration profile.\n",
    "    \"\"\"\n",
    "    z_coords, rho = np.loadtxt(xvg_file, comments=[\"@\", \"#\"], unpack=True)\n",
    "    mid_idx = int(len(rho) / 2 - 1)\n",
    "    c_profile = 0.5 * (rho[:mid_idx + 1][::-1] + rho[mid_idx:-1])  # in mol/L\n",
    "    \n",
    "    z_coords = z_coords[:mid_idx + 1]  # in nm\n",
    "\n",
    "    return z_coords, c_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, c_original = compute_c_profile(\"density_test.txt\")\n",
    "# z, c_1 = compute_c_profile(\"density_hw_r1_288.xvg\")  # replicate 1\n",
    "# z, c_2 = compute_c_profile(\"density_hw_r2_288.xvg\")  # replicate 2\n",
    "# z, c_3 = compute_c_profile(\"density_hw_r3_288.xvg\")  # replicate 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List concentration profiles\n",
    "concentration_profiles = np.array([c_original,c_1,c_2,c_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from Dr Shirts: It is more rigorous to normalize the densities, since we know what the total number must be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eq 13: $$ L_{x} L_{y} \\int^{L_{z}/2}_{-L_{z}/2} C_{s}(z) dz = N_{s}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the total number of ions, N_s (using Equation 13)\n",
    "\n",
    "for c in concentration_profiles:\n",
    "    N_s = L_x * L_y * simpson(c, z)\n",
    "    print(N_s)  # this should compare to number of ions used (3m used 188)\n",
    "    c /= (N_s/Ntrue)\n",
    "    \n",
    "# # verify normalization worked.\n",
    "# for c in concentration_profiles:\n",
    "#     N_s = L_x * L_y * simpson(c, z)\n",
    "#     print(N_s)  # compare to number of ions used (3m used 188)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note BM: could maybe code up a funciton to check the above ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the concentration profiles (original and replicates)\n",
    "plt.figure()\n",
    "for i, c in enumerate(concentration_profiles):\n",
    "    plt.plot(z, c, label=\"Replicate \"+str(i))\n",
    "plt.xlabel(\"Z (nm)\")\n",
    "plt.ylabel(\"Concentration (mol/L)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating eq 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ln \\left[ \\frac{C_{s}(z)}{C_{max}} \\right] + A \\left[ \\frac{C_{max}^{1/2}}{1+BC_{max}^{1/2}} - \\frac{C_{s}(z)^{1/2}}{1+BC_{s}(z)^{1/2}} \\right] + \\sum^{i_{max}}_{i=1} \\alpha_{i} [C_{s}(z)^{i} - C_{max}^{i}] = - \\frac {U(z)}{RT}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_y(theta, x):\n",
    "    \"\"\"\n",
    "    Calculates -U(z) / RT using Equation 12.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : list\n",
    "        A list containing the three parameters to be fit, including B, alpha_1, alpha_2.\n",
    "    x : concentration list\n",
    "    \"\"\"\n",
    "    A = 1.7964  # in M^(-1/2), value according to the SI\n",
    "\n",
    "    # Converting the units for A so that they are nm-based\n",
    "    conversion_factor = 1E24 / NA  # 1 nm^(-3) ~= 1.6605 M\n",
    "    A /= (conversion_factor ** 0.5)\n",
    "\n",
    "    C_max = np.max(x)\n",
    "    term_1 = np.log(x / C_max)\n",
    "    term_2 = -A * np.sqrt(x) / (1 + theta[0] * np.sqrt(x))\n",
    "    term_3 = A * np.sqrt(C_max) / (1 + theta[0] * np.sqrt(C_max))\n",
    "    term_4 = theta[1]* (x - C_max) \n",
    "    term_5 = theta[2] * (x ** 2 - C_max ** 2)\n",
    "    y = term_1 + term_2 + term_3 + term_4 + term_5\n",
    "\n",
    "    return y\n",
    "\n",
    "def residuals(theta, x, y):\n",
    "    return np.power(calc_y(theta, x) - y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the direct average rather than over bootstrap means\n",
    "mean_profile = np.mean(concentration_profiles,axis=0)\n",
    "c_0 = mean_profile\n",
    "\n",
    "# Removing 0 values from the concentration profile, and the corresponding z values\n",
    "z_fit = z[c_0 != 0]\n",
    "c_fit = c_0[c_0 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 1.7964  # in M^(-1/2), value according to the SI\n",
    "\n",
    "# Converting the units for A so that they are nm-based\n",
    "conversion_factor = 1E24 / NA  # 1 nm^(-3) ~= 1.6605 M\n",
    "A /= (conversion_factor ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right side/ y of equation 12\n",
    "y_values = -(0.5 * k * z_fit ** 2) * 1000 / (R * T) \n",
    "\n",
    "#Getting fit for full left side of eq 12\n",
    "result_unweighted = least_squares(residuals, [4, 0.2, 0], args=(c_fit, y_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting unweighted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the unweighted results\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(z_fit, y_values, 'b', label=\"-U(z)/RT\")\n",
    "plt.plot(z_fit, np.log(c_fit/np.max(c_fit)), 'g--', label=\"Ideal\")\n",
    "plt.plot(z_fit, calc_y(result_unweighted.x,c_fit), 'r--', label=\"Fitting eq 12\")\n",
    "plt.xlabel('z (nm)')\n",
    "plt.ylabel('-U(z)/RT')\n",
    "plt.title('Fitting of eq. 12 using 3 replicates with 144 samples each')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see reasonably good fit - the idea curve (green line) is corrected to the be closer to the blue (red line is closer tothe blue line). However, we would also like to not fit the noise at the end. \n",
    "\n",
    "To do this, we do some error propagation to find the ideal weights for the least square fit. $C(z)/C_{max}$ is a probability, or at least proportional to a probability.  The uncertainty in a probability estimate $\\hat{p}$ from a histogram (whose value with infinitely collected points would be $p$), can be shown to be $\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$, where $\\hat{p}$ is the measured probability in that histogram bin, and $n$ is the total number of points collected over all bins.  But we are interested in the error of $\\ln \\hat{p}$. Generally the error in function $f$ of random variable $x$ $\\delta f(x)$ is equal to $|f'(x)| \\delta x$. So $\\delta (\\ln p) = \\frac{\\delta p}{p} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n\\hat{p}^2}}$.\n",
    "\n",
    "For weighted least squares, it can be show that the weights should be proportional to 1/variance of the data point. The variance will be $(\\delta\\ln p)^2 = \\frac{\\hat{p}(1-\\hat{p})}{n\\hat{p}^2} = \\frac{1-\\hat{p}}{np}$.  The weights are only defined up to a constant, so we can just use $\\frac{1-p}{p}$.  To do this, we have to normalize $C/C_{max}$, which is easy to do, we call it K. So the weights should be proportional to $\\frac{C/K}{1-C/K} = \\frac{C}{K-C}$.  So the most well defined points will get the most probability, the noisiest points the least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find weights for least square fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = N_s/(L_x*L_y)\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_residuals(theta, x, y):\n",
    "    return (x/(norm-x))*np.power(calc_y(theta, x) - y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting fit for full left side of eq 12\n",
    "result_weighted = least_squares(weighted_residuals, [4, 0.2,-0.01], args=(c_fit, y_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(z_fit, y_values, 'b', label=\"-U(z)/RT\")\n",
    "plt.plot(z_fit, np.log(c_fit/np.max(c_fit)), 'g--', label=\"Ideal\")\n",
    "plt.plot(z_fit, calc_y(result_unweighted.x,c_fit), 'r--', label=\"Unweighted Fitting eq 12\")\n",
    "plt.plot(z_fit, calc_y(result_weighted.x,c_fit), 'm--', label=\"Weighted Fitting eq 12\")\n",
    "plt.xlabel('z (nm)')\n",
    "plt.ylabel('-U(z)/RT')\n",
    "plt.title('Fitting of eq. 12 using 3 replicates with 144 samples each')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to see what is going on, so let's look at differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results. Notice the plots are now being subtracted from the y_values results\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(z_fit, y_values-np.log(c_fit/np.max(c_fit)), 'g--', label=\"Ideal\")\n",
    "plt.plot(z_fit, y_values-calc_y(result_unweighted.x,c_fit), 'r--', label=\"Unweighted Fitting eq 12\")\n",
    "plt.plot(z_fit, y_values-calc_y(result_weighted.x,c_fit), 'm--', label=\"Weighted Fitting eq 12\")\n",
    "plt.xlabel('z (nm)')\n",
    "plt.ylabel('-U(z)/RT')\n",
    "plt.title('Fitting of eq. 12 using 3 replicates with 144 samples each')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted fitting is much closer to zero over the range with reasonable concentrations, say z=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zlim = 4 #coordinate limit for where fitting is closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stdev of unweighted results\n",
    "np.std(y_values[z_fit<zlim]-calc_y(result_unweighted.x,c_fit[z_fit<zlim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stdev of weighted results\n",
    "np.std(y_values[z_fit<zlim]-calc_y(result_weighted.x,c_fit[z_fit<zlim]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the distribution of the parameters with the fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"B =\", result_unweighted.x[0])\n",
    "print(\"alpha1 =\", result_unweighted.x[1])\n",
    "print(\"alpha2 =\", result_unweighted.x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"B =\", result_weighted.x[0])\n",
    "print(\"alpha1 =\", result_weighted.x[1])\n",
    "print(\"alpha2 =\", result_weighted.x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we expect that the parameters will be more consistent between runs with weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping Profiles (without and with weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialp = [4, 0.1,0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrapping\n",
    "def bootstrap_profiles(profiles, n_samples, res_func):\n",
    "    bootstrap_params = []\n",
    "    n_profiles = len(profiles)\n",
    "    for _ in range(n_samples):\n",
    "        # Resample with replacement\n",
    "        bootstrap_sample = [profiles[np.random.randint(0, n_profiles)] for _ in range(n_profiles)]\n",
    "        # Calculate mean profile\n",
    "        mean_profile = np.mean(bootstrap_sample, axis=0)\n",
    "        z_fit = z[mean_profile != 0]\n",
    "        c_fit = mean_profile[mean_profile != 0] \n",
    "        y_values = -(0.5 * k * z_fit ** 2) * 1000 / (R * T)\n",
    "        result = least_squares(res_func, trialp, args=(c_fit, y_values))\n",
    "        bootstrap_params.append(result.x)\n",
    "        \n",
    "    return np.array(bootstrap_params) # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform bootstrapping with unweighted fit\n",
    "bootstrap_params_unweighted = bootstrap_profiles(profiles=concentration_profiles, n_samples=500, res_func=residuals)\n",
    "\n",
    "for i in range(len(trialp)):\n",
    "    pd = bootstrap_params_unweighted[:,i]\n",
    "    std_param = np.std(pd) \n",
    "    mean_param = np.mean(pd)  # it's actually more accurate to\n",
    "                             # just use the single fits, but we don't \n",
    "                             # necessarily know the name of that results structure\n",
    "                             # when we are at this point in the code, use the mean for now\n",
    "    print(f\"Param {i} = {mean_param} +/- {std_param}\")\n",
    "    plt.hist(pd)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform bootstrapping with weighted residuals\n",
    "n_bootstrap = 500\n",
    "bootstrap_params_weighted  = bootstrap_profiles(profiles=concentration_profiles, n_samples=500, res_func=weighted_residuals)\n",
    "\n",
    "for i in range(len(trialp)):\n",
    "    pd = bootstrap_params_weighted[:,i]\n",
    "    std_param = np.std(pd) \n",
    "    mean_param = np.mean(pd)  # it's actually more accurate to\n",
    "                             # just use the single fits, but we don't \n",
    "                             # necessarily know the name of that results structure\n",
    "                             # when we are at this point in the code, use the mean for now\n",
    "    print(f\"Param {i} = {mean_param} +/- {std_param}\")\n",
    "    plt.hist(pd)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more consistent performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try maximum likelihood parameter optimization.  The basic idea is to express the probability of parameters given the data, $P(a|x)$, and using Bayes' rule to find that \n",
    "\n",
    "$P(a|x) = \\frac{P(x|a)P(a)}{P(x)}$\n",
    "\n",
    "We want to find the parameters \n",
    "$P(x)$ does not depend on the parameters. If we assume no prior information about the parameters $P(a)$ (that would be Bayesian inference), then we have.\n",
    "\n",
    "$P(a|x) \\propto P(x|a)$\n",
    "\n",
    "So we need to maximize the probability given the data $P(\\vec{x}|a)$. Assuming the samples are independent, then the total probability given all of the data is $\\prod_{i=1}^N P(x_i|a)$.  \n",
    "\n",
    "Since the logarithm is a monotonic function, then maximizing $\\ln P(\\vec{x}|a) = \\sum_{i=1}^N \\ln P(x_i|a)$ is the same as maximizing $P(\\vec{x}|a$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the problem to the one of the chemical potential of ions trapped in a harmonic potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ion_array.bin', 'rb') as f:\n",
    "    zs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ionz = np.abs(np.array(zs)-72)/10 # rescale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zvals = ionz.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we are essentially finding the parameters that examine the probability of finding an ion at a given $x$, since concentration is proportional to probability.  \n",
    "\n",
    "We convert the concentration $C(z)/C_{max}$ to an unnormalized probability $p(z)$.  \n",
    "\n",
    "$\\ln p(z|B,\\alpha_1,\\alpha_2) \\propto \\ln \\frac{C(z)}{C_{max}}$\n",
    "\n",
    "$\\ln \\frac{C(z)}{C_{max}} = \\sum_i -\\frac{kz^2}{2RT} + A\\left(\\frac{C(z)^{1/2}}{1+BC(z)^{1/2}} - \\frac{C_{max}^{1/2}}{1+BC_{max}^{1/2}} \\right) -\\alpha_1(C(z)-C_{max}) - \\alpha_2(C(z)^2-C_{max}^2)$  \n",
    "\n",
    "\n",
    "\n",
    "$C_{max}$ and also the normalizing constant will be fixed by the fact that we have a fixed number of ions present.\n",
    "\n",
    "This is actually not ideal for maximizing, since the concentration $c(z)$ occurs on both sides. It would be great if we could algebraically solve for $c(z)$, but this does not appear to be possible.\n",
    "\n",
    "We can't actually leave out any of the terms, because all of the terms involve the parameters implicitly (including $C_{max}$).\n",
    "\n",
    "We approach the problem the following way:\n",
    "\n",
    " - Given a set $[B,\\alpha_1,\\alpha_2]$, find the concentration $C(z|B,\\alpha_1,\\alpha_2)$ that satisfies those parameters. \n",
    " - This will need to be done self-consistently, as each time the parameters change, $C_{max}$ will also change.\n",
    " - We will make use of a reference ideal distribution with no parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the ideal solution function which satisfies the normalization and find the $C_{max}$ for this solution, i.e. \n",
    "\n",
    "$N_p = L_x L_y C_{max} \\int_0^{\\infty} p(z|B,\\alpha_1,\\alpha_2) dz$\n",
    "\n",
    "$C_{max} = \\frac{ N_p }{L_x L_y} \\left(\\int_0^{\\infty} p(z|B,\\alpha_1,\\alpha_2\\right) dz )^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad, simpson, trapezoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_U_app = lambda z: 1000*k*z*z/(2*R*T) #left side of eq 12\n",
    "cz_ideal = lambda z: np.exp(-red_U_app(z)) \n",
    "\n",
    "#Should not hard code k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_norm = quad(cz_ideal,0,10)[0]\n",
    "print(ideal_norm)\n",
    "ideal_norm = np.sqrt(np.pi*R*T/(2*1000*k))\n",
    "print(ideal_norm)\n",
    "\n",
    "#are we checking these both yield approx the same?\n",
    "#where did the second equation come from? Why pi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmax_ideal=N_s/(L_x*L_y*ideal_norm)\n",
    "print(cmax_ideal)\n",
    "\n",
    "#same as cell above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue is doing the integration of the model parameter to normalize it. \n",
    "\n",
    "We could try to take advantage of importance sampling - if we have two unnormalized probability distributions $q_i$ and $q_j$ ($p_i$ normalized), and we have samples from $q_i$, then we can have\n",
    "\n",
    "$1 = \\int p_j(z) dz \\\\\n",
    "= \\int \\frac{p_j(z)}{p_i(z)}p_i(z) \\\\\n",
    "= \\int \\frac{q_j(z)/Z_j}{q_i(z)/Z_i} p_i(z) \\\\\n",
    "= \\int \\frac{Z_i}{Z_j} \\frac{q_j(z)}{q_i(z)} p_i(z) \\\\\n",
    "\\approx \\frac{Z_i}{Z_j} \\frac{1}{N} \\sum_n \\frac{q_j(z_n)}{q_i(z_n)} \\\\\n",
    "\\frac{Z_i}{Z_j} \\approx \\frac{1}{N} \\sum_n \\frac{q_j(z_n)}{q_i(z_n)}\n",
    "$\n",
    "\n",
    " So if we know the ideal solution normalizing factor, we can find\n",
    "\n",
    "$Z_{trial} = Z_{ideal} \\frac{1}{N} \\sum_n \\frac{q_{ideal}(z)}{q_{trial}(z)} =  Z_{ideal} \\left \\langle \\frac{q_{ideal}(z)}{q_{trial}(z)} \\right \\rangle$\n",
    "\n",
    "However, this doesn't actually appear work if we using this equation to optimize the parameters, because if we have a mismatch between the trial function and the data sampled; if the trial function functional form doesn't actually match the PDF measured, then we will continually get worse results.\n",
    "\n",
    "Let's do something silly/simple and assume we have enough data, and just take the points that are output, and do Simpson's rule on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that iterates the calculation of a concentration.\n",
    "def czfunc(a,z,cz):\n",
    "    # exponential of eq 12 for ln c/cmax for the data points\n",
    "    cznew = np.exp(logprobfunc(a,z,cz))\n",
    "    # these indices do not need to be repeated if this is expensive,\n",
    "    # but if we bootstrap, they will change, so keep for now. \n",
    "    zunique,locs = np.unique(z,return_index=True)\n",
    "    # now we have sorted unique points, we can integrate\n",
    "    newarea = simpson(cznew[locs],zunique)\n",
    "    cznorm_ratio = ideal_norm/newarea\n",
    "    return (cmax_ideal*cznorm_ratio)*cznew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprobfunc(a,z,cz,A=A):\n",
    "    # we need to calculate the maximum concentration \n",
    "    cmax = np.max(cz) # this only works if a sample is at z=0: improve!                      \n",
    "    czh = np.sqrt(cz)\n",
    "    cmaxh = np.sqrt(cmax)\n",
    "    term_1 = -red_U_app(z)\n",
    "    term_2 = A*((czh/(1+a[0]*czh))-(cmaxh/(1+a[0]*cmaxh)))\n",
    "    term_3 = - a[1]*(cz - cmax)\n",
    "    term_4 = - a[2]*(cz**2 - cmax**2)\n",
    "    return term_1 + term_2 + term_3 + term_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converge_c(params, z, cz_start,\n",
    "               niter=1000,lim=0.0001,verbose=False):\n",
    "\n",
    "    # hot start from ideal, otherwise use a good guess.\n",
    "    if isinstance(cz_start,str):\n",
    "        if cz_start == 'ideal':\n",
    "            cz_start = cmax_ideal*cz_ideal(z)\n",
    "            \n",
    "    c_old = cz_start\n",
    "    for i in range(niter):\n",
    "        c_new = czfunc(params,z,c_old)\n",
    "        norm = np.sqrt(np.dot(c_new-c_old,c_new-c_old))\n",
    "        if verbose:\n",
    "            print(i,norm)\n",
    "        c_old = c_new\n",
    "        if norm < lim:\n",
    "            break\n",
    "\n",
    "    return czfunc(params,z,c_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many duplicates of each $z$ value are there?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zsparse,zcount=np.unique(zvals,return_counts=True)\n",
    "plt.plot(zsparse,zcount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, quite a few!  We can take advantage of that; each time the same $z$ is processed, it would result in the same number, so we can calculate the contribution for each $z$ once and then mulitply by the number of times it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newp = np.array([1.35,0.33,-0.04])\n",
    "c_new = converge_c(newp,zvals,cz_start='ideal',lim=0.001,verbose=True)\n",
    "plt.scatter(zvals,c_new,s=0.1,lw=0.1,c='m', label='zvals vs c_new')\n",
    "plt.scatter(zvals,cmax_ideal*cz_ideal(zvals),s=0.1,lw=0.1,c='b', label='zvals vs cmax(id)*cz(id)')\n",
    "plt.plot(z,concentration_profiles[0],'k',lw=0.5, label='original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can converge faster with sparse data. (?)\n",
    "newp = np.array([1.35,0.33,-0.04])\n",
    "zsparse,zcount=np.unique(zvals,return_counts=True)\n",
    "c_new_sparse = converge_c(newp,zsparse,cz_start='ideal',lim=0.001,verbose=True)\n",
    "plt.scatter(zsparse,c_new_sparse,s=0.1,lw=0.1,c='m', label='zvals vs c_new')\n",
    "plt.scatter(zsparse,cmax_ideal*cz_ideal(zsparse),s=0.1,lw=0.1,c='b', label='zvals vs cmax(id)*cz(id)')\n",
    "plt.plot(z,concentration_profiles[0],'k',lw=0.5, label='original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neglliter(a,z,cz_start,counts=None,lim=0.0001,verbose=False,print_val=False,doeval=False):\n",
    "    if len(z)!= len(cz_start):\n",
    "        print(\"x and y data does not match length\")\n",
    "        return\n",
    "    if doeval == True:\n",
    "        lim = 1\n",
    "    # generate the new concentration with these parameters\n",
    "    cz = converge_c(a,z,cz_start,lim=lim,verbose=verbose)\n",
    "    # divide by cmax_ideal to make the numbers smaller, \n",
    "    # it's a constant so it doesn't affect the results     \n",
    "    terms = np.log(cz/cmax_ideal)\n",
    "    # if we don't know how many of each value there are,\n",
    "    # just add them all up\n",
    "    if counts is None:\n",
    "        csum = np.sum(terms)\n",
    "    else:\n",
    "        # if we DO know how many of each value there are\n",
    "        # then just do each value once, and multiply\n",
    "        # by the number of counts\n",
    "        csum = np.dot(counts,terms)\n",
    "    if print_val:\n",
    "        print(\"csum\",csum)\n",
    "    return -1*csum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summing over all the values is slow\n",
    "start = timer()\n",
    "results = minimize(neglliter,newp,args=(zvals,c_new),method='Nelder-Mead',options={'maxiter':200})\n",
    "# this one also works pretty well\n",
    "#results = minimize(neglliter,newp,args=(zvals,c_new),method='COBYLA',options={'rhobeg':0.001})\n",
    "end = timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "print(results)\n",
    "c_opt = converge_c(results.x,zvals,cz_start=c_new,lim=0.0001,verbose=False)\n",
    "optp = results.x\n",
    "print(optp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yep, we have reduced the negative log likelihood, \n",
    "# i.e. maximized the likelihood\n",
    "print(neglliter(newp,zvals,c_new,doeval=True))\n",
    "print(neglliter(optp,zvals,c_opt,doeval=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the improved solution looks like! (red line vs magenta line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(zvals,c_new,s=0.1,lw=0.1,c='m')\n",
    "plt.plot(z,concentration_profiles[0],'k')\n",
    "plt.scatter(zvals,cmax_ideal*cz_ideal(zvals),s=0.1,lw=0.1,c='b')\n",
    "plt.scatter(zvals,c_opt,s=0.1,lw=0.1,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we can make this much faster by noting that there are a lot of repeats of each value, so we can simplify the sum (MIGHT be slightly different because of rounding).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zsparse,zcount=np.unique(zvals,return_counts=True)\n",
    "plt.plot(zsparse,zcount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is now much faster, and gives essentially the same answer.\n",
    "start = timer()\n",
    "results = minimize(neglliter,newp,args=(zsparse,c_new_sparse,zcount),method='Nelder-Mead')\n",
    "end = timer()\n",
    "print(\"time=\",end-start)\n",
    "print(results)\n",
    "full_opt = results.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now plot the new data \n",
    "c_opt_sparse = converge_c(results.x,zsparse,cz_start=c_new_sparse,verbose=False)\n",
    "plt.scatter(zsparse,c_new_sparse,s=0.1,lw=0.1,c='m')\n",
    "plt.plot(z,concentration_profiles[0],'k')\n",
    "plt.scatter(zsparse,cmax_ideal*cz_ideal(zsparse),s=0.1,lw=0.1,c='b')\n",
    "plt.scatter(zsparse,c_opt_sparse,s=0.1,lw=0.1,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the parameters are somewhat different, but the curves are about the same; the space appears to be relatively degenerate, in that there's a large volume of parameters that all give similar results. But that's probably OK for osmotic coefficients, as there will also be many parameters that thus give about the same osmotic coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work!  Things to do:\n",
    "\n",
    "1. Implement bootstrapping. A key here is one can now bootstrap over all the SAMPLES (like 188,000), not the profiles (4).\n",
    "2. See what happens when you use all the data from the trajectories.  You can just dump them all into one big array - the bootstrapping is on samples, not on the trajectories.\n",
    "3. See if you can get away with 2 paramters, not 3, and still get a good fit.  \n",
    "4. Bootstrap around the osmotic coefficient/pressure calculation given the curve - this will be much less noisy than the parameters themselves. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrapping over _samples_\n",
    "def bootstrap_samples(samples, n_boot, minfunc, init_params):\n",
    "    zinit=np.unique(samples)\n",
    "    c_start = converge_c(init_params,zinit,cz_start='ideal',verbose=False)\n",
    "    bootstrap_params = list()\n",
    "    bootstrap_cs = list()\n",
    "    n_samples = len(samples)\n",
    "    for i in range(n_boot):\n",
    "        # Resample with replacement\n",
    "        indices = np.random.randint(0,n_samples,size=n_samples)\n",
    "        bootstrap_sample = samples[indices]\n",
    "        zsparse,zcount=np.unique(bootstrap_sample,return_counts=True)\n",
    "        # we can't use the old concentrations since they are at potentially different z\n",
    "        # so interpolate (should be negligible error)\n",
    "        c_new = np.interp(zsparse,zinit,c_start)\n",
    "        result = minimize(minfunc,init_params,args=(zsparse,c_new,zcount),method=\"Nelder-Mead\")\n",
    "        print(i,result.x,result.fun)\n",
    "        bootstrap_params.append(result.x)\n",
    "        # generate a converged concentration at this point. Make the locations\n",
    "        # at the same points as the original\n",
    "        c_new = converge_c(result.x,zsparse,cz_start=c_new)\n",
    "        c_new = np.interp(zinit,zsparse,c_new)\n",
    "        bootstrap_cs.append(c_new) \n",
    "    return np.array(bootstrap_params), np.array(bootstrap_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bparams,bcs = bootstrap_samples(zvals,500,neglliter,results.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot the new data \n",
    "for i in range(len(bcs)):    \n",
    "    plt.scatter(zsparse,bcs[i],s=0.1,lw=0.1,c='m')\n",
    "plt.plot(z,concentration_profiles[0],'k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation coefficients\n",
    "stdparam = np.std(bparams,axis=0)\n",
    "mxy = np.outer(stdparam,stdparam) #<x><y>\n",
    "covparam = np.cov(bparams.T,ddof=0) # <xy>\n",
    "#print(covparam)\n",
    "covcorr = covparam/mxy\n",
    "print(covcorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that parameters are highly correlated, as you can see by the correlation matrix  - there are multiple parameters that give essentially the same result. It may be a 2 parameter fit is enough!  Bootstrapping over osmotic coefficients should give less noisy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bparams[0,:])):\n",
    "    pd = bparams[:,i]\n",
    "    std_param = np.std(pd) \n",
    "    mean_param = np.mean(pd)  # it's actually more accurate to\n",
    "                             # just use the single fits, but we don't \n",
    "                             # necessarily know the name of that results structure\n",
    "                             # when we are at this point in the code, use the mean for now\n",
    "    print(f\"Parma {i} = {mean_param} +/- {std_param}\")\n",
    "    plt.hist(pd)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the standard deviations of the parameters are lower with the maximum likelihood approximation.\n",
    "\n",
    "B: \n",
    "weighted: 1.33 +/- 0.65\n",
    "ML: 2.54 +/- 0.44\n",
    "\n",
    "$\\alpha_1$: \n",
    "weighted: 0.357 +/- 0.127\n",
    "ML: 0.223 +/- 0.025\n",
    "\n",
    "$\\alpha_2$: \n",
    "weighted: -0.0317 +/- 0.0102\n",
    "ML:  -0.0205 +/- 0.0041\n",
    "\n",
    "Note that it is OK that the uncertainties are not within each other, since this is a highly correlated problem, so there are multiple solutions - small change in optimization can lead to large changes in the parameters while still being relatively close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def osmotic_pressure(cs,params, nu=2):\n",
    "    # takes in a concentration, model parameters and nu. \n",
    "    term1 = cs\n",
    "    term2 = params[1]*0.5*cs**2\n",
    "    term3 = params[2]*(1/3.0)*cs**3\n",
    "    csh = np.sqrt(cs)\n",
    "    B = params[0]\n",
    "    Bf = 1+B*csh\n",
    "    term4 = (A/B**2)*((2/B)*np.log(Bf) - (2*csh+B*cs)/Bf)    \n",
    "    p = nu*R*T*(term1+term2+term3+term4)\n",
    "    ip = nu*R*T*cs  # this should be the ideal osmotic pressure\n",
    "    return p,p/ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op1m=osmotic_pressure(0.98,bparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plots of the osmotic coefficients and osmotic pressures\n",
    "ops = list()\n",
    "ocs = list()\n",
    "for p,c in zip(bparams,bcs):\n",
    "    op,oc = osmotic_pressure(c,p)\n",
    "    plt.scatter(c,oc,s=0.1,lw=0.1,alpha=0.01,c='m')\n",
    "    ops.append(op)\n",
    "    ocs.append(oc)\n",
    "opt_oc = osmotic_pressure(c_opt_sparse,full_opt)[1]\n",
    "plt.plot(c_opt_sparse,opt_oc,'k',lw=0.5,)\n",
    "ops = np.array(ops)\n",
    "ocs = np.array(ocs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = np.percentile(ocs, 2.5, axis=0)\n",
    "upper_bound = np.percentile(ocs, 97.5, axis=0)\n",
    "# Plotting the results\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(9.0,10.0))\n",
    "plt.plot(c_opt_sparse, opt_oc)\n",
    "plt.fill_between(c_opt_sparse, lower_bound, upper_bound, color='b', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.xlabel(\"Concentration (M)\", fontsize=MEDIUM_SIZE)\n",
    "plt.ylabel('Osmotic Coefficients', fontsize=MEDIUM_SIZE)\n",
    "plt.legend()\n",
    "plt.title('Maximum Likelihood Parameter Fitting', fontsize=BIGGER_SIZE)\n",
    "plt.savefig('oc.png', dpi=1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(c_opt_sparse), opt_oc)\n",
    "plt.fill_between(np.log(c_opt_sparse), lower_bound, upper_bound, color='b', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.xlabel('Log(Concentration)')\n",
    "plt.ylabel('Osmotic Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I extract the osmotic values at each concentration?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
